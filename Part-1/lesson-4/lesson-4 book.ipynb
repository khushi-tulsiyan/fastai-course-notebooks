{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders =['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once again Mr. Costner has dragged out a movie for far longer than necessary'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = files[0].open().read(); \n",
    "txt[:76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#187) ['Once','again','Mr.','Costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','Aside','from','the','terrific','sea','rescue','sequences',',','of','which','there','are','very','few','I'...]\n"
     ]
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) ['The','U.S.','dollar','lis1.00']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(spacy(['The U.S. dollar lis1.00']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(tkn('©   Fast.ai www.fast.ai/INDEX'), 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "txts = L(o.open().read() for o in files[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return \" \".join(first(sp([txt]))[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.10% [2/2000 00:00&lt;00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁O n ce ▁again ▁M r . ▁Co st n er ▁has ▁ d ra g g ed ▁out ▁a ▁movie ▁for ▁far ▁long er ▁than ▁ ne ce s s ar y . ▁A side ▁from ▁the ▁ ter'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁ O n ce ▁a g a in ▁ M r . ▁ C o s t n er ▁ h a s ▁ d ra g g ed ▁ o u t ▁a ▁movie ▁for ▁f ar ▁ l'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁On ce ▁again ▁Mr . ▁Costner ▁has ▁dragged ▁out ▁a ▁movie ▁for ▁far ▁longer ▁than ▁necessary . ▁A side ▁from ▁the ▁terrific ▁sea ▁rescue ▁sequences , ▁of ▁which ▁there ▁are ▁very ▁few ▁I ▁just ▁did ▁not ▁care ▁about ▁any ▁of'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP-2: Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#146) ['xxbos','xxmaj','cage','plays','a','drunk','and','gets','high','critically','praise','.','xxmaj','elizabeth','xxmaj','shue','xxmaj','actually','has','to'...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#1968) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','of','to','is','it','i','in'...]\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,  349,  183,    8, 1177,   10,    8, 1178,   60, 1455,\n",
       "              62,   12,   25,   28,  189,  957,   93,  958,   10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj once again xxmaj mr . xxmaj costner has dragged out a movie for far longer than necessary .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hide_input\n",
    "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
    "tokens = tkn(stream)\n",
    "bs,seq_len = 6,15\n",
    "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums200 = toks200.map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = LMDataLoader(nums200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = first(dl)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj once again xxmaj mr . xxmaj costner has dragged out a movie for far longer than necessary .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP -3: Training a Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\INDIA\\\\.fastai\\\\data\\\\imdb_tok\\\\counter.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dls_lm \u001b[38;5;241m=\u001b[39m DataBlock(\n\u001b[1;32m----> 2\u001b[0m     blocks \u001b[38;5;241m=\u001b[39m TextBlock\u001b[38;5;241m.\u001b[39mfrom_folder(path, is_lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      3\u001b[0m     get_items \u001b[38;5;241m=\u001b[39m get_imdb, splitter \u001b[38;5;241m=\u001b[39m RandomSplitter(\u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mdataloaders(path, path \u001b[38;5;241m=\u001b[39m path, bs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\INDIA\\anaconda3\\Lib\\site-packages\\fastai\\text\\data.py:244\u001b[0m, in \u001b[0;36mTextBlock.from_folder\u001b[1;34m(cls, path, vocab, is_lm, seq_len, backwards, min_freq, max_vocab, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;129m@delegates\u001b[39m(Tokenizer\u001b[38;5;241m.\u001b[39mfrom_folder, keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_folder\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path, vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, is_lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m72\u001b[39m, backwards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, max_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60000\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild a `TextBlock` from a `path`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(Tokenizer\u001b[38;5;241m.\u001b[39mfrom_folder(path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), vocab\u001b[38;5;241m=\u001b[39mvocab, is_lm\u001b[38;5;241m=\u001b[39mis_lm, seq_len\u001b[38;5;241m=\u001b[39mseq_len,\n\u001b[0;32m    245\u001b[0m                backwards\u001b[38;5;241m=\u001b[39mbackwards, min_freq\u001b[38;5;241m=\u001b[39mmin_freq, max_vocab\u001b[38;5;241m=\u001b[39mmax_vocab)\n",
      "File \u001b[1;32mc:\\Users\\INDIA\\anaconda3\\Lib\\site-packages\\fastai\\text\\core.py:284\u001b[0m, in \u001b[0;36mTokenizer.from_folder\u001b[1;34m(cls, path, tok, rules, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: tok \u001b[38;5;241m=\u001b[39m WordTokenizer()\n\u001b[0;32m    283\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m tokenize_folder(path, tok\u001b[38;5;241m=\u001b[39mtok, rules\u001b[38;5;241m=\u001b[39mrules, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 284\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(tok, counter\u001b[38;5;241m=\u001b[39mload_pickle(output_dir\u001b[38;5;241m/\u001b[39mfn_counter_pkl),\n\u001b[0;32m    285\u001b[0m           lengths\u001b[38;5;241m=\u001b[39mload_pickle(output_dir\u001b[38;5;241m/\u001b[39mfn_lengths_pkl), rules\u001b[38;5;241m=\u001b[39mrules, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    286\u001b[0m res\u001b[38;5;241m.\u001b[39mpath,res\u001b[38;5;241m.\u001b[39moutput_dir \u001b[38;5;241m=\u001b[39m path,output_dir\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\INDIA\\anaconda3\\Lib\\site-packages\\fastcore\\xtras.py:242\u001b[0m, in \u001b[0;36mload_pickle\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad a pickle file from a file name or opened file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m open_file(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32mc:\\Users\\INDIA\\anaconda3\\Lib\\site-packages\\fastcore\\xtras.py:230\u001b[0m, in \u001b[0;36mopen_file\u001b[1;34m(fn, mode, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mreturn\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mGzipFile(fn, mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(fn, mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fn,mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\INDIA\\\\.fastai\\\\data\\\\imdb_tok\\\\counter.pkl'"
     ]
    }
   ],
   "source": [
    "dls_lm = DataBlock(\n",
    "    blocks = TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items = get_imdb, splitter = RandomSplitter(0.1),\n",
    ").dataloaders(path, path = path, bs=128, seq_len=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dls_lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dls_lm\u001b[38;5;241m.\u001b[39mshow_batch(max_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dls_lm' is not defined"
     ]
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP-4: Fine Tuning the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dls_lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m learn \u001b[38;5;241m=\u001b[39m language_model_learner(\n\u001b[1;32m----> 2\u001b[0m     dls_lm, AWD_LSTM, drop_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m      3\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[accuracy, Perplexity()])\u001b[38;5;241m.\u001b[39mto_fp16()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dls_lm' is not defined"
     ]
    }
   ],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3,\n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m learn\u001b[38;5;241m.\u001b[39mfit_one_cycle(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2e-2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING AND LOADING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m learn\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "learn.save('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m learn \u001b[38;5;241m=\u001b[39m learn\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encode('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m n_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      5\u001b[0m n_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 7\u001b[0m preds \u001b[38;5;241m=\u001b[39m [learn\u001b[38;5;241m.\u001b[39mpredict(text, n_words, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m)\n\u001b[0;32m      8\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_sentences)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"I am learning NLP and it is a bit hard but i like it.\"\n",
    "\n",
    "n_words = 50\n",
    "\n",
    "n_sentences = 3\n",
    "\n",
    "preds = [learn.predict(text, n_words, temperature=0.75)\n",
    "         for _ in range(n_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastai in c:\\users\\india\\anaconda3\\lib\\site-packages (2.7.18)\n",
      "Requirement already satisfied: pip in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (24.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (24.1)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.8,>=1.5.29 in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (1.7.28)\n",
      "Requirement already satisfied: torchvision>=0.11 in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (0.20.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (3.9.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (2.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (2.32.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (6.0.1)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (1.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (10.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (1.13.1)\n",
      "Requirement already satisfied: spacy<4 in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (3.8.3)\n",
      "Requirement already satisfied: torch<2.6,>=1.10 in c:\\users\\india\\anaconda3\\lib\\site-packages (from fastai) (2.5.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (8.3.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (1.26.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (75.1.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\india\\anaconda3\\lib\\site-packages (from requests->fastai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\india\\anaconda3\\lib\\site-packages (from requests->fastai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from requests->fastai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\india\\anaconda3\\lib\\site-packages (from requests->fastai) (2024.12.14)\n",
      "Requirement already satisfied: filelock in c:\\users\\india\\anaconda3\\lib\\site-packages (from torch<2.6,>=1.10->fastai) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from torch<2.6,>=1.10->fastai) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\india\\anaconda3\\lib\\site-packages (from torch<2.6,>=1.10->fastai) (3.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\india\\anaconda3\\lib\\site-packages (from torch<2.6,>=1.10->fastai) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from torch<2.6,>=1.10->fastai) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<2.6,>=1.10->fastai) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\india\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\india\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from pandas->fastai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\india\\anaconda3\\lib\\site-packages (from pandas->fastai) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from scikit-learn->fastai) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from scikit-learn->fastai) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\india\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\india\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.16.0)\n",
      "Requirement already satisfied: blis<1.2.0,>=1.1.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy<4->fastai) (1.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy<4->fastai) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\india\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4->fastai) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\india\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from jinja2->spacy<4->fastai) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\india\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install fastai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dls_lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dls_clas \u001b[38;5;241m=\u001b[39m DataBlock(\n\u001b[1;32m----> 2\u001b[0m     blocks\u001b[38;5;241m=\u001b[39m(TextBlock\u001b[38;5;241m.\u001b[39mfrom_folder(path, vocab\u001b[38;5;241m=\u001b[39mdls_lm\u001b[38;5;241m.\u001b[39mvocab),CategoryBlock),\n\u001b[0;32m      3\u001b[0m     get_y \u001b[38;5;241m=\u001b[39m parent_label,\n\u001b[0;32m      4\u001b[0m     get_items\u001b[38;5;241m=\u001b[39mpartial(get_text_files, folders\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      5\u001b[0m     splitter\u001b[38;5;241m=\u001b[39mGrandparentSplitter(valid_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39mdataloaders(path, path\u001b[38;5;241m=\u001b[39mpath, bs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m72\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dls_lm' is not defined"
     ]
    }
   ],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=128, seq_len=72)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samp = toks200[:10].map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [207,314,267,378,304,156,197,187,162,229]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_samp.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
